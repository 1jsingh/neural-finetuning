{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import scipy\n",
    "\n",
    "# misc utility functions\n",
    "from utils.misc import *\n",
    "\n",
    "# load model definitions\n",
    "from utils.models import *\n",
    "\n",
    "# sklearn\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# ignore convergence warnings from sklearn\n",
    "import warnings\n",
    "import sklearn.exceptions\n",
    "warnings.filterwarnings(\"ignore\", category=sklearn.exceptions.ConvergenceWarning)\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "# pandas\n",
    "import pandas as pd\n",
    "\n",
    "# matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# seaborn\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "# pickle\n",
    "import pickle\n",
    "\n",
    "# cv2\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data\n",
    "\n",
    "The neural-activation (*in pickle format*) data consists of an organized dictionary with the following entries:\n",
    "\n",
    "* `images_paths`: list containing paths to all the 1960 images\n",
    "* `image_ctg`: numpy array containing class labels from 0 -> 6\n",
    "* `image_splits` : 1960 x 10 numpy array containing 10 80:20 train:val splits used in the paper. Though I generate my own validation splits for computing the sit scores\n",
    "* `features`: 168 dimensional(for multi-unit) neural_features for all the images i.e 1960 x 168 numpy array\n",
    "* `categ_name_map`: dictionary mapping from numeric class label to class name e.g. face, animal etc.\n",
    "\n",
    "The dataset consists of images belonging to 7 classes and 49 object types. The image paths are arranged in an order such that the images belonging to a particular object type are together. There are 40 images per object in the dataset, so images [1 - 40] belong to object 1, images [41 - 80] belong to object 2 and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/PLoSCB2014_data_20141216'\n",
    "with open('data/PLoSCB2014_data_20141216/NeuralData_IT_multiunits.pkl','rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Input Images\n",
    "\n",
    "For feeding the cadieu dataset images to the pretrained CNNs, we need to preprocess the images with appropriate reshaping, normalization and other data augmentation steps. In addition, we also need to convert the images to tensors, in order to use pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 1960 images ... preprocessed input shape: torch.Size([1960, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# define normalize transform to be used while feeding images to the pretrained CNN\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "\n",
    "# combine of transforms in a composition\n",
    "transform = transforms.Compose([\n",
    "            transforms.Resize(size=(224,224)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ])\n",
    "\n",
    "# preprocessed input images list\n",
    "X = []\n",
    "\n",
    "for i,img_path in enumerate(data['image_paths']):\n",
    "    img = transform(Image.open(os.path.join(data_path,img_path)))\n",
    "    X.append(img)\n",
    "\n",
    "# convert the list into a tensor\n",
    "X = torch.stack(X)\n",
    "\n",
    "print (\"read {} images ... preprocessed input shape: {}\".format(X.shape[0],X.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the pretrained model\n",
    "\n",
    "There are 2 steps to be done here:\n",
    "\n",
    "* Load the pretrained model e.g. alexnet,vgg16,resnet50 etc.\n",
    "* Change it appropriately in order to extract appropriate features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = Alexnet_partial()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read neural features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read neural features for 1960 images with shape: (1960, 168)\n"
     ]
    }
   ],
   "source": [
    "neural_features = data['features']\n",
    "print (\"read neural features for {} images with shape: {}\".format(neural_features.shape[0],neural_features.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datagenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Datagenerator:\n",
    "    def __init__(self,imgs,neural_features,val_ratio=0.2):\n",
    "        \n",
    "        # get train/val split using val_ratio\n",
    "        train_mask,val_mask = get_train_val_split_indices(total_num_imgs=imgs.shape[0],val_ratio=val_ratio)\n",
    "        \n",
    "        # get training and validation data\n",
    "        self.imgs_train,self.neural_feat_train = imgs[train_mask],neural_features[train_mask]\n",
    "        self.imgs_val,self.neural_feat_val = imgs[val_mask],neural_features[val_mask]\n",
    "        \n",
    "        # number of objects\n",
    "        self.num_obj=49\n",
    "        \n",
    "    \n",
    "    def get_next(self,batch_size=49*2,mode='train'):\n",
    "        \n",
    "        # check if batch size is multiple of self.num_obj\n",
    "        assert batch_size%self.num_obj == 0\n",
    "        \n",
    "        if mode == 'train':\n",
    "            img_split,neural_feat_split = self.imgs_train,self.neural_feat_train\n",
    "        else:\n",
    "            img_split,neural_feat_split = self.imgs_val,self.neural_feat_val\n",
    "        \n",
    "        # compute batch_size ratio\n",
    "        batch_size_ratio = batch_size/img_split.shape[0]\n",
    "        \n",
    "        # sample batch indices\n",
    "        _,batch_mask = get_train_val_split_indices(total_num_imgs=img_split.shape[0],val_ratio=batch_size_ratio)\n",
    "            \n",
    "        # get batch imgs and neural features\n",
    "        img_batch,neural_feat_batch = img_split[batch_mask],neural_feat_split[batch_mask]\n",
    "        \n",
    "        return img_batch,neural_feat_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune the pretrained model using RDM loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self,model,device,learning_rate):\n",
    "        # define model and transfer to device\n",
    "        self.model = model.to(device)\n",
    "        \n",
    "        # define optimizer\n",
    "        self.opt = optim.Adam(self.model.parameters(),lr=learning_rate)\n",
    "        \n",
    "        # define datagenerator for sampling batches\n",
    "        self.datagen = Datagenerator(X,neural_features,val_ratio=0.2)\n",
    "        \n",
    "    \n",
    "    def train(self,batch_size=49*8,max_train_steps=1000,print_every=10):\n",
    "        # set model to train mode\n",
    "        self.model.train()\n",
    "            \n",
    "        for train_step in range(max_train_steps):\n",
    "\n",
    "            # sample training batch\n",
    "            img_batch,neural_feat_batch = self.datagen.get_next(batch_size=batch_size)\n",
    "\n",
    "            # compute rdm from neural features\n",
    "            neural_rdm = get_rdm_tensor(torch.from_numpy(neural_feat_batch).float().to(device))\n",
    "\n",
    "            # get model_features\n",
    "            model_features = self.model(img_batch.to(device))\n",
    "            # compute model_rdm\n",
    "            model_rdm = get_rdm_tensor(model_features)\n",
    "\n",
    "            # define rdm loss\n",
    "            loss = torch.mean((model_rdm-neural_rdm)**2)\n",
    "\n",
    "            # set optimizer grad to 0\n",
    "            self.opt.zero_grad()\n",
    "            # do backprop on loss\n",
    "            loss.backward()\n",
    "            # perform optimizer step\n",
    "            self.opt.step()\n",
    "            \n",
    "            if train_step%print_every == 0:\n",
    "                self.model.eval()\n",
    "                #train_model_features = self.model(img_batch.to(device)).detach().cpu().numpy()\n",
    "                #train_sit_mean,train_sit_std = sit_score(train_model_features,neural_feat_batch,\n",
    "                #                                                     num_val_splits=1,val_ratio=1)\n",
    "                \n",
    "                rdm_nn = neural_rdm.cpu().numpy()\n",
    "                rdm_model = model_rdm.detach().cpu().numpy()\n",
    "                \n",
    "                # get upper triangular matrix values for model and neural rdm\n",
    "                iu1 = np.triu_indices(49,k=1)\n",
    "                rdm_neural_triu = rdm_nn[iu1]\n",
    "                rdm_model_triu = rdm_model[iu1]\n",
    "                \n",
    "                train_sit_mean = scipy.stats.spearmanr(rdm_model_triu,rdm_neural_triu).correlation\n",
    "                \n",
    "                val_model_features = extract_features(self.datagen.imgs_val,self.model)\n",
    "                val_sit_mean,val_sit_std = sit_score(val_model_features,self.datagen.neural_feat_val,\n",
    "                                                                     num_val_splits=1,val_ratio=1)\n",
    "                \n",
    "                self.model.train()\n",
    "                print (\"Train step: {}\\t loss: {:.3f}\\t train_sit: {:.4f} \\t val_sit: {:.4f}\".format(train_step,\n",
    "                                                                        loss.item(),train_sit_mean,val_sit_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train step: 0\t loss: 10.090\t train_sit: 0.5865 \t val_sit: 0.0423\n",
      "Train step: 100\t loss: 1.581\t train_sit: 0.4240 \t val_sit: 0.0418\n",
      "Train step: 200\t loss: 1.088\t train_sit: 0.6580 \t val_sit: -0.0031\n",
      "Train step: 300\t loss: 1.113\t train_sit: 0.7031 \t val_sit: 0.0217\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-03e84d16deb0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m49\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_train_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-21-7511c3899acf>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, batch_size, max_train_steps, print_every)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;31m# sample training batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mimg_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mneural_feat_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatagen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_next\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;31m# compute rdm from neural features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-7fcb7472a8f3>\u001b[0m in \u001b[0;36mget_next\u001b[0;34m(self, batch_size, mode)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# get batch imgs and neural features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mimg_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mneural_feat_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_split\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mneural_feat_split\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mneural_feat_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = Alexnet_partial()\n",
    "\n",
    "agent = Agent(model,device,learning_rate=1e-3)\n",
    "agent.train(batch_size=49*8,max_train_steps=2000,print_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_features = extract_features(X,agent.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get linear svm accuracy for model features \n",
    "acc_mean,acc_std = linear_svm_score(model_features,data['image_ctg'],neural_features)\n",
    "print (\"Model features: linear svm accuracy mean: {} \\t std: {}\".format(acc_mean,acc_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv3",
   "language": "python",
   "name": "cv3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
