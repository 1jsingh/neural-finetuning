{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import scipy\n",
    "\n",
    "# misc utility functions\n",
    "import utils.misc as misc\n",
    "\n",
    "# load model definitions\n",
    "from utils.loader import ModelLoader\n",
    "\n",
    "# sklearn\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# ignore convergence warnings from sklearn\n",
    "import warnings\n",
    "import sklearn.exceptions\n",
    "warnings.filterwarnings(\"ignore\", category=sklearn.exceptions.ConvergenceWarning)\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "# pandas\n",
    "import pandas as pd\n",
    "\n",
    "# matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# seaborn\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "# pickle\n",
    "import pickle\n",
    "\n",
    "# cv2\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data\n",
    "\n",
    "The neural-activation (*in pickle format*) data consists of an organized dictionary with the following entries:\n",
    "\n",
    "* `images_paths`: numpy array containing paths to all the 1960 images\n",
    "* `image_ctg`: numpy array containing class labels from 0 -> 6\n",
    "* `image_splits` : 1960 x 10 numpy array containing 10 80:20 train:val splits used in the paper. Though I generate my own validation splits for computing the sit scores\n",
    "* `features`: 168 dimensional(for multi-unit) neural_features for all the images i.e 1960 x 168 numpy array\n",
    "* `categ_name_map`: dictionary mapping from numeric class label to class name e.g. face, animal etc.\n",
    "\n",
    "The dataset consists of images belonging to 7 classes and 49 object types. The image paths are arranged in an order such that the images belonging to a particular object type are together. There are 40 images per object in the dataset, so images [1 - 40] belong to object 1, images [41 - 80] belong to object 2 and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/PLoSCB2014_data_20141216'\n",
    "with open('data/PLoSCB2014_data_20141216/NeuralData_IT_multiunits.pkl','rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Input Images\n",
    "\n",
    "For feeding the cadieu dataset images to the pretrained CNNs, we need to preprocess the images with appropriate reshaping, normalization and other data augmentation steps. In addition, we also need to convert the images to tensors, in order to use pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 1960 images ... preprocessed input shape: torch.Size([1960, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# read cadieu dataset images\n",
    "X = misc.read_images(data['image_paths'],data_path=data_path)\n",
    "\n",
    "print (\"read {} images ... preprocessed input shape: {}\".format(X.shape[0],X.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read neural features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read neural features for 1960 images with shape: (1960, 168)\n"
     ]
    }
   ],
   "source": [
    "neural_features = data['features']\n",
    "print (\"read neural features for {} images with shape: {}\".format(neural_features.shape[0],neural_features.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datagenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Datagenerator:\n",
    "    def __init__(self,imgs,neural_features,val_ratio=0.2):\n",
    "        \n",
    "        # get train/val split using val_ratio\n",
    "        self.train_mask,self.val_mask = misc.get_train_val_split_indices(total_num_imgs=imgs.shape[0],val_ratio=val_ratio)\n",
    "        \n",
    "        # get training and validation data\n",
    "        self.imgs_train,self.neural_feat_train = imgs[self.train_mask],neural_features[self.train_mask]\n",
    "        self.imgs_val,self.neural_feat_val = imgs[self.val_mask],neural_features[self.val_mask]\n",
    "        \n",
    "        # number of objects\n",
    "        self.num_obj=49\n",
    "    \n",
    "    def get_next(self,batch_size=49*2,mode='train'):\n",
    "        \n",
    "        # check if batch size is multiple of self.num_obj\n",
    "        assert batch_size%self.num_obj == 0\n",
    "        \n",
    "        if mode == 'train':\n",
    "            img_split,neural_feat_split = self.imgs_train,self.neural_feat_train\n",
    "        else:\n",
    "            img_split,neural_feat_split = self.imgs_val,self.neural_feat_val\n",
    "        \n",
    "        # compute batch_size ratio\n",
    "        batch_size_ratio = batch_size/img_split.shape[0]\n",
    "        \n",
    "        # sample batch indices\n",
    "        _,batch_mask = misc.get_train_val_split_indices(total_num_imgs=img_split.shape[0],val_ratio=batch_size_ratio)\n",
    "            \n",
    "        # get batch imgs and neural features\n",
    "        img_batch,neural_feat_batch = img_split[batch_mask],neural_feat_split[batch_mask]\n",
    "        \n",
    "        return img_batch,neural_feat_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune the pretrained model using RDM loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self,model_name,rdm_gt_feat,device,learning_rate):\n",
    "        # model loader instance \n",
    "        model_loader = ModelLoader()\n",
    "        \n",
    "        # define model and transfer to device\n",
    "        self.model = model_loader.load(model_name).to(device)\n",
    "        \n",
    "        # define optimizer\n",
    "        self.opt = optim.Adam(self.model.parameters(),lr=learning_rate)\n",
    "        \n",
    "        # define datagenerator for sampling batches\n",
    "        self.datagen = Datagenerator(X,rdm_gt_feat,val_ratio=0.2)\n",
    "        \n",
    "\n",
    "    def train(self,batch_size=49*8,max_train_steps=1000,print_every=10):\n",
    "        # set model to train mode\n",
    "        self.model.train()\n",
    "            \n",
    "        for train_step in range(max_train_steps):\n",
    "\n",
    "            # sample training batch\n",
    "            img_batch,neural_feat_batch = self.datagen.get_next(batch_size=batch_size)\n",
    "            img_batch = img_batch.to(device)\n",
    "            \n",
    "            # compute rdm from neural features\n",
    "            neural_rdm = misc.get_rdm(neural_feat_batch)\n",
    "            neural_rdm = torch.from_numpy(neural_rdm).float().to(device)\n",
    "\n",
    "            # get model_features and compute model rdm\n",
    "            model_features = self.model(img_batch).squeeze()\n",
    "            model_rdm = misc.get_rdm_tensor(model_features)\n",
    "\n",
    "            # define rdm loss\n",
    "            loss = torch.mean((model_rdm-neural_rdm)**2)\n",
    "\n",
    "            # perform optimization step\n",
    "            self.opt.zero_grad()\n",
    "            loss.backward()\n",
    "            self.opt.step()\n",
    "            \n",
    "            # print progress after every 'print_every' steps\n",
    "            if train_step%print_every == 0:\n",
    "                \n",
    "                # set model to eval mode\n",
    "                self.model.eval()\n",
    "                \n",
    "                # compute sit for training batch\n",
    "                train_model_features = misc.extract_features(img_batch.detach().cpu(),self.model,batch_size=4)\n",
    "                train_sit_mean,train_sit_std = misc.sit_score(train_model_features,neural_feat_batch,\n",
    "                                                                    num_val_splits=1,val_ratio=1)\n",
    "                \n",
    "                # compute sit for validation dataset\n",
    "                val_model_features = misc.extract_features(self.datagen.imgs_val,self.model)\n",
    "                val_sit_mean,val_sit_std = misc.sit_score(val_model_features,self.datagen.neural_feat_val,\n",
    "                                                                     num_val_splits=1,val_ratio=1)\n",
    "                \n",
    "                # compute linear svm accuracy for validation dataset\n",
    "                model_features = misc.extract_features(agent.datagen.imgs_val,agent.model) \n",
    "                acc_mean,acc_std = misc.linear_svm_score_v2(model_features,data['image_ctg'][agent.datagen.val_mask],\n",
    "                                                    num_val_splits=20,num_subsampled_feat=-1)\n",
    "                \n",
    "                # set model to train mode\n",
    "                self.model.train()\n",
    "                \n",
    "                # print evaluation metric values\n",
    "                print (\"Step: {}\\t loss: {:.3f}\\t train_sit: {:.4f} \\t val_sit: {:.4f} \\t lsvm_acc: {:.4f}\".format(train_step,\n",
    "                                                                loss.item(),train_sit_mean,val_sit_mean,acc_mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model features for RDM ground truth computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model to be used for rdm training ground truth\n",
    "## CHANGE THIS\n",
    "rdm_model_name = 'resnet34'\n",
    "\n",
    "# dir storing model features\n",
    "model_feat_path = 'models/model_features/'\n",
    "\n",
    "with open(model_feat_path+rdm_model_name+'_feat.pkl','rb') as f:\n",
    "    rdm_model_feat = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## DECIDE features to be used for rdm ground truth computation\n",
    "rdm_gt_feat = neural_features\n",
    "#rdm_gt_feat = rdm_model_feat\n",
    "\n",
    "# create agent instance\n",
    "agent = Agent(model_name='squeezenet_v0',rdm_gt_feat=rdm_gt_feat,device=device,learning_rate=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-training: validation model features\t lsvm_acc_mean: 0.7476\t lsvm_acc_std: 0.0602\n"
     ]
    }
   ],
   "source": [
    "# test linear svm accuracy on the validation dataset before training\n",
    "\n",
    "# model features for validation images\n",
    "model_features = misc.extract_features(agent.datagen.imgs_val,agent.model)\n",
    "\n",
    "# get linear svm accuracy for validation model features \n",
    "acc_mean,acc_std = misc.linear_svm_score_v2(model_features,data['image_ctg'][agent.datagen.val_mask],\n",
    "                                        num_val_splits=100,num_subsampled_feat=-1)\n",
    "print (\"Pre-training: validation model features\\t lsvm_acc_mean: {:.4f}\\t lsvm_acc_std: {:.4f}\".format(acc_mean,acc_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0\t loss: 0.632\t train_sit: 0.4429 \t val_sit: 0.5780 \t lsvm_acc: 0.7439\n",
      "Step: 100\t loss: 0.461\t train_sit: 0.4229 \t val_sit: 0.6114 \t lsvm_acc: 0.7510\n",
      "Step: 200\t loss: 0.316\t train_sit: 0.4546 \t val_sit: 0.6302 \t lsvm_acc: 0.7510\n",
      "Step: 300\t loss: 0.192\t train_sit: 0.5308 \t val_sit: 0.6703 \t lsvm_acc: 0.7724\n",
      "Step: 400\t loss: 0.156\t train_sit: 0.5226 \t val_sit: 0.6949 \t lsvm_acc: 0.7582\n",
      "Step: 500\t loss: 0.119\t train_sit: 0.4862 \t val_sit: 0.7217 \t lsvm_acc: 0.7816\n",
      "Step: 600\t loss: 0.080\t train_sit: 0.5707 \t val_sit: 0.7241 \t lsvm_acc: 0.7735\n",
      "Step: 700\t loss: 0.075\t train_sit: 0.5830 \t val_sit: 0.7280 \t lsvm_acc: 0.7816\n",
      "Step: 800\t loss: 0.066\t train_sit: 0.5530 \t val_sit: 0.7329 \t lsvm_acc: 0.7898\n",
      "Step: 900\t loss: 0.056\t train_sit: 0.6019 \t val_sit: 0.7350 \t lsvm_acc: 0.8041\n"
     ]
    }
   ],
   "source": [
    "# train the agent\n",
    "agent.train(batch_size=49*4,max_train_steps=1000,print_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-training: validation model features\t lsvm_acc_mean: 0.7637\t lsvm_acc_std: 0.0614\n"
     ]
    }
   ],
   "source": [
    "# test linear svm accuracy on the validation dataset after training\n",
    "\n",
    "# model features for validation images\n",
    "model_features = misc.extract_features(agent.datagen.imgs_val,agent.model)\n",
    "\n",
    "# get linear svm accuracy for validation model features \n",
    "acc_mean,acc_std = misc.linear_svm_score_v2(model_features,data['image_ctg'][agent.datagen.val_mask],\n",
    "                                        num_val_splits=100,num_subsampled_feat=-1)\n",
    "print (\"Post-training: validation model features\\t lsvm_acc_mean: {:.4f}\\t lsvm_acc_std: {:.4f}\".format(acc_mean,acc_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
